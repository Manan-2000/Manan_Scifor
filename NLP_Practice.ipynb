{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXdMt567R4wr",
        "outputId": "bed130c5-bbd6-4d87-b2ef-ddf0f1c14521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stem1=PorterStemmer()\n",
        "from nltk.stem import wordnet\n",
        "lemma=wordnet.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(txt):\n",
        "  return txt.lower()\n",
        "inp_str=\"Weather is too Cloudy. Possibility of Rain is High, Today!!\"\n",
        "print(lowercase(inp_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEiOhRdPSolv",
        "outputId": "ceece11b-129a-4dda-f249-7080c4c7c48e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weather is too cloudy. possibility of rain is high, today!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_num(txt):\n",
        "  return re.sub(r'\\d+','',txt)\n",
        "inp=\"You bought 6 candies from shop, and 4 candies are in home.\"\n",
        "print(remove_num(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71bxJ02WTB2x",
        "outputId": "0d1f9eae-4c75-42fa-fe8f-5e8ee50a98c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bought  candies from shop, and  candies are in home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num2word(txt):\n",
        "  temp=txt.split()\n",
        "  new_txt=[]\n",
        "  for i in temp:\n",
        "    if i.isdigit():\n",
        "      new_txt.append(inflect.engine().number_to_words(i))\n",
        "    else:\n",
        "      new_txt.append(i)\n",
        "  temp=' '.join(new_txt)\n",
        "  return temp\n",
        "\n",
        "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
        "print(num2word(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgjcVCAIT3wZ",
        "outputId": "1824c950-2935-43ab-96da-018423a2d440"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bought six candies from shop, and four candies are in home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(txt):\n",
        "  t=str.maketrans('','',string.punctuation)\n",
        "  return txt.translate(t)\n",
        "\n",
        "inp_str=\"Hey, are you excited?? After a week, we will be in Shimla!!\"\n",
        "print(remove_punc(inp_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6N4VVktYpdf",
        "outputId": "8c453486-1800-4649-a161-9ac4e33b20d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey are you excited After a week we will be in Shimla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  word_tokens=word_tokenize(text)\n",
        "  filtered_sentence=[]\n",
        "  for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "  return(\" \".join(filtered_sentence))\n",
        "eg_txt=\"Data is the new oil. AI is the last invention\"\n",
        "print(remove_stopwords(eg_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35w11NsRZMdm",
        "outputId": "d865f59f-1f0a-4603-b812-e28683d24c29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data new oil . AI last invention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "def stem_words(text):\n",
        "  word_tokens=word_tokenize(text)\n",
        "  stem=[stem1.stem(i) for i in word_tokens]\n",
        "  return(stem)\n",
        "text=\"Data is a new revolution in the world, in a day one individual would generate terabytes of data.\"\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSH7omZEaP49",
        "outputId": "2d3ba802-2e67-42a0-d846-24b9ca7018de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "def lemma_words(text):\n",
        "  word_tokens=word_tokenize(text)\n",
        "  lemm=[lemma.lemmatize(i) for i in word_tokens]\n",
        "  return(lemm)\n",
        "text=\"Data is a new revolution in the world, in a day one individual would generate terabytes of data.\"\n",
        "lemma_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoW6pUJTaUEm",
        "outputId": "c6bee80c-3952-4ea0-a1bb-e77beb63eb92"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'new',\n",
              " 'revolution',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individual',\n",
              " 'would',\n",
              " 'generate',\n",
              " 'terabyte',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tagging(text):\n",
        "  word_token=word_tokenize(text)\n",
        "  return (pos_tag(word_token))\n",
        "print(pos_tagging('Are you afraid of somehing?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7piyFdcaw2W",
        "outputId": "19d81803-fe07-48bd-b278-bf8fc4a0eb37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Are', 'NNP'), ('you', 'PRP'), ('afraid', 'IN'), ('of', 'IN'), ('somehing', 'VBG'), ('?', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunking(text,grammer):\n",
        "  word_token=word_tokenize(text)\n",
        "  word_pos=pos_tag(word_token)\n",
        "  chunk_parser=nltk.RegexpParser(grammer)\n",
        "  tree=chunk_parser.parse(word_pos)\n",
        "  for i in tree.subtrees():\n",
        "    print(i)\n",
        "    #tree.draw()\n",
        "sentence='the little red parrot is flying in the sky'\n",
        "grammer=\"NP:{<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence,grammer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpBFrcnfbI6O",
        "outputId": "c9eb496c-10b8-4d42-dbda-699c8f5f5de2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ner(text):\n",
        "  word_token=word_tokenize(text)\n",
        "  word_pos=pos_tag(word_token)\n",
        "  print(ne_chunk(word_pos))\n",
        "text='Brain Lara scored the highest 400 runs in a Test match which was played between West Indies and England in 2017.'\n",
        "ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX5jhoy3bSuO",
        "outputId": "52201f2d-5ee9-4be4-93d9-860f9c9767f4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  (GPE Test/NNP)\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  was/VBD\n",
            "  played/VBN\n",
            "  between/IN\n",
            "  (GPE West/NNP Indies/NNPS)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  in/IN\n",
            "  2017/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\", sent)\n",
        "print(r2)\n",
        "r3=re.findall(r'^\\w',sent)\n",
        "print(r3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1amzRYpDcCPe",
        "outputId": "96d2132f-ed86-4934-a1db-4ed163419e6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n",
            "['d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((re.split(r'\\s','we splitted this sentence')))\n",
        "print((re.split(r's','we splitted this sentence')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TidUbQUcN-9",
        "outputId": "8af90e99-32f0-404d-b6e4-a13e27d7004f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'splitted', 'this', 'sentence']\n",
            "['we ', 'plitted thi', ' ', 'entence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst=['icecream images','i immitated','inner peace']\n",
        "for i in lst:\n",
        "  a=re.match(\"(i\\w+)\\W(i\\w+)\",i)\n",
        "  if a:\n",
        "    print((a.groups()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-fhoy7CcQ-d",
        "outputId": "996c481a-0d9d-49ef-e371-575bfb91f3a2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('icecream', 'images')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern=[\"playing\",\"dataset\"]\n",
        "text=\"Raju is playing outside\"\n",
        "for p in pattern:\n",
        "  print(\"you are looking for '{}' in '{}'\".format(p,text),end='  ')\n",
        "  if re.search(p,text):\n",
        "    print(\"Found a match!\")\n",
        "  else:\n",
        "    print(\"No match\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxSmmjrdcvEX",
        "outputId": "6f494b44-b16c-4a93-a3de-506fd5fb38de"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are looking for 'playing' in 'Raju is playing outside'  Found a match!\n",
            "you are looking for 'dataset' in 'Raju is playing outside'  No match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=\"abc@gmail.com,xyz@gmail.com,lmn@gmail.com,efg@gmail.com\"\n",
        "y=re.findall(r'[\\w\\.-]+@[\\w\\.-]+',x)\n",
        "for i in y:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfaARZfcycM",
        "outputId": "4d998a7d-6ece-4421-cb45-97626883126b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc@gmail.com\n",
            "xyz@gmail.com\n",
            "lmn@gmail.com\n",
            "efg@gmail.com\n"
          ]
        }
      ]
    }
  ]
}