{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXdMt567R4wr",
        "outputId": "bcb092a7-3eab-49ee-90de-9021ecb1cb08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stem1=PorterStemmer()\n",
        "from nltk.stem import wordnet\n",
        "lemma=wordnet.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Python String\n",
        "a='data'\n",
        "print(a)\n",
        "\n",
        "#String are arrays\n",
        "a=\"Dataset\"\n",
        "print(a[4])\n",
        "\n",
        "#Slicing\n",
        "a=\"Dataset\"\n",
        "print(a[2:5])\n",
        "\n",
        "#Negative Indexing\n",
        "a=\"Data Set\"\n",
        "print(a[-6:-2])\n",
        "\n",
        "#Methods\n",
        "print(a.split(\"a\"))\n",
        "print(a.lower())\n",
        "print(a.upper())\n",
        "print(a.replace('a','e'))\n",
        "\n",
        "#String Concatination\n",
        "b=\"Train\"\n",
        "print(a+\" \"+b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKmYoP7yeK1m",
        "outputId": "e6862b6f-a8f1-4756-ec75-bc2a31eca8d2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\n",
            "s\n",
            "tas\n",
            "ta S\n",
            "['D', 't', ' Set']\n",
            "data set\n",
            "DATA SET\n",
            "Dete Set\n",
            "Data Set Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a text file\n",
        "f=open(\"data.txt\",\"w+\")#w+ means write and read mode\n",
        "for i in range(5):\n",
        "  f.write(\"This is line %d\\n\" % (i+1))\n",
        "f.close()\n",
        "\n",
        "#Append data to a file\n",
        "f=open(\"data.txt\",\"a+\")#a+ means if the file doesn't exist create one\n",
        "for i in range(3):\n",
        "  f.write(\"Appending is line %d\\n\" % (i+1))\n",
        "f.close()\n",
        "\n",
        "#Read file\n",
        "r=open(\"data.txt\",\"r\")\n",
        "if r.mode=='r':\n",
        "  content=r.read()\n",
        "  print(content)\n",
        "  print(\"This is the output\")\n",
        "r.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDKIKNWoeN4V",
        "outputId": "c3f5dd2d-129d-4e0e-991f-483658653909"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is line 1\n",
            "This is line 2\n",
            "This is line 3\n",
            "This is line 4\n",
            "This is line 5\n",
            "Appending is line 1\n",
            "Appending is line 2\n",
            "Appending is line 3\n",
            "\n",
            "This is the output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Lowercase\n",
        "def lowercase(txt):\n",
        "  return txt.lower()\n",
        "inp_str=\"Weather is too Cloudy. Possibility of Rain is High, Today!!\"\n",
        "print(lowercase(inp_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEiOhRdPSolv",
        "outputId": "f7200d73-82c7-4ac3-e109-1a7871f8198f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weather is too cloudy. possibility of rain is high, today!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove numbers using Regular Expressions\n",
        "def remove_num(txt):\n",
        "  return re.sub(r'\\d+','',txt)\n",
        "inp=\"You bought 6 candies from shop, and 4 candies are in home.\"\n",
        "print(remove_num(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71bxJ02WTB2x",
        "outputId": "c983b389-1737-44ed-b307-f562fd6e050d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bought  candies from shop, and  candies are in home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Numbers to Words\n",
        "def num2word(txt):\n",
        "  temp=txt.split()\n",
        "  new_txt=[]\n",
        "  for i in temp:\n",
        "    if i.isdigit():\n",
        "      new_txt.append(inflect.engine().number_to_words(i))\n",
        "    else:\n",
        "      new_txt.append(i)\n",
        "  temp=' '.join(new_txt)\n",
        "  return temp\n",
        "\n",
        "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
        "print(num2word(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgjcVCAIT3wZ",
        "outputId": "af5ce7a4-3e41-4d60-c377-694326345236"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You bought six candies from shop, and four candies are in home.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Punctuations\n",
        "def remove_punc(txt):\n",
        "  t=str.maketrans('','',string.punctuation)\n",
        "  return txt.translate(t)\n",
        "\n",
        "inp_str=\"Hey, are you excited?? After a week, we will be in Shimla!!\"\n",
        "print(remove_punc(inp_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6N4VVktYpdf",
        "outputId": "56b7ba31-be7c-40e6-d8c6-2af90f8d3444"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey are you excited After a week we will be in Shimla\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Stopwords\n",
        "def remove_stopwords(text):\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  word_tokens=word_tokenize(text)\n",
        "  filtered_sentence=[]\n",
        "  for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "      filtered_sentence.append(w)\n",
        "  return(\" \".join(filtered_sentence))\n",
        "eg_txt=\"Data is the new oil. AI is the last invention\"\n",
        "print(remove_stopwords(eg_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35w11NsRZMdm",
        "outputId": "c3d8c2d0-6a31-445d-a420-546eb0c3c116"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data new oil . AI last invention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "def stem_words(text):\n",
        "  word_tokens=word_tokenize(text)\n",
        "  stem=[stem1.stem(i) for i in word_tokens]\n",
        "  return(stem)\n",
        "text=\"Data is a new revolution in the world, in a day one individual would generate terabytes of data.\"\n",
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSH7omZEaP49",
        "outputId": "ebe8fede-5de4-4003-e506-9c24931aba08"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "def lemma_words(text):\n",
        "  word_tokens=word_tokenize(text)\n",
        "  lemm=[lemma.lemmatize(i) for i in word_tokens]\n",
        "  return(lemm)\n",
        "text=\"Data is a new revolution in the world, in a day one individual would generate terabytes of data.\"\n",
        "lemma_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoW6pUJTaUEm",
        "outputId": "c03c8974-567b-4e5b-d046-8bbc808b6777"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'is',\n",
              " 'a',\n",
              " 'new',\n",
              " 'revolution',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individual',\n",
              " 'would',\n",
              " 'generate',\n",
              " 'terabyte',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parts of speech tagging\n",
        "def pos_tagging(text):\n",
        "  word_token=word_tokenize(text)\n",
        "  return (pos_tag(word_token))\n",
        "print(pos_tagging('Are you afraid of somehing?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7piyFdcaw2W",
        "outputId": "032951f7-4959-4643-f607-5cf55ae29fc8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Are', 'NNP'), ('you', 'PRP'), ('afraid', 'IN'), ('of', 'IN'), ('somehing', 'VBG'), ('?', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunking\n",
        "def chunking(text,grammer):\n",
        "  word_token=word_tokenize(text)\n",
        "  word_pos=pos_tag(word_token)\n",
        "  chunk_parser=nltk.RegexpParser(grammer)\n",
        "  tree=chunk_parser.parse(word_pos)\n",
        "  for i in tree.subtrees():\n",
        "    print(i)\n",
        "sentence='the little red parrot is flying in the sky'\n",
        "grammer=\"NP:{<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence,grammer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpBFrcnfbI6O",
        "outputId": "d1173d5b-bbb5-42ec-a315-28d72e35d87a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Name entity recognition\n",
        "def ner(text):\n",
        "  word_token=word_tokenize(text)\n",
        "  word_pos=pos_tag(word_token)\n",
        "  print(ne_chunk(word_pos))\n",
        "text='Brain Lara scored the highest 400 runs in a Test match which was played between West Indies and England in 2017.'\n",
        "ner(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX5jhoy3bSuO",
        "outputId": "4e30d08a-2f83-45d7-adb7-e41bcca8d3cc"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  (GPE Test/NNP)\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  was/VBD\n",
            "  played/VBN\n",
            "  between/IN\n",
            "  (GPE West/NNP Indies/NNPS)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  in/IN\n",
            "  2017/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Regular Expressions\n",
        "sent = \"dataset, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\", sent)\n",
        "print(r2)\n",
        "r3=re.findall(r'^\\w',sent)\n",
        "print(r3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1amzRYpDcCPe",
        "outputId": "73ffdfc2-7224-4db4-c59b-e7560f860254"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset']\n",
            "['d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\\s expression in re.split\n",
        "print((re.split(r'\\s','we splitted this sentence')))\n",
        "print((re.split(r's','we splitted this sentence')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TidUbQUcN-9",
        "outputId": "bd7f79f3-d85d-437a-f117-fc55f57feb5e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'splitted', 'this', 'sentence']\n",
            "['we ', 'plitted thi', ' ', 'entence']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using re.match\n",
        "lst=['icecream images','i immitated','inner peace']\n",
        "for i in lst:\n",
        "  a=re.match(\"(i\\w+)\\W(i\\w+)\",i)\n",
        "  if a:\n",
        "    print((a.groups()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-fhoy7CcQ-d",
        "outputId": "194a6e65-d6f0-4afb-a611-9e380332a9e9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('icecream', 'images')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding pattern in the text(re.search)\n",
        "pattern=[\"playing\",\"dataset\"]\n",
        "text=\"Raju is playing outside\"\n",
        "for p in pattern:\n",
        "  print(\"you are looking for '{}' in '{}'\".format(p,text),end='  ')\n",
        "  if re.search(p,text):\n",
        "    print(\"Found a match!\")\n",
        "  else:\n",
        "    print(\"No match\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxSmmjrdcvEX",
        "outputId": "0f7996e5-1444-4461-dad2-6fb98472e4a2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are looking for 'playing' in 'Raju is playing outside'  Found a match!\n",
            "you are looking for 'dataset' in 'Raju is playing outside'  No match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using re.findall\n",
        "x=\"abc@gmail.com,xyz@gmail.com,lmn@gmail.com,efg@gmail.com\"\n",
        "y=re.findall(r'[\\w\\.-]+@[\\w\\.-]+',x)\n",
        "for i in y:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfaARZfcycM",
        "outputId": "78b7bd1b-5cb3-4110-d7b1-efbc8a134319"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abc@gmail.com\n",
            "xyz@gmail.com\n",
            "lmn@gmail.com\n",
            "efg@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pe0qw8RleEpe"
      },
      "execution_count": 60,
      "outputs": []
    }
  ]
}